model:
  name: "gpt2"
  max_length: 256

training:
  output_dir: "./fine_tuned_model"
  learning_rate: 2e-5
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 8
  num_train_epochs: 3
  weight_decay: 0.01
  save_total_limit: 2

dataset:
  name: "daily_dialog"
  max_samples: 10000
